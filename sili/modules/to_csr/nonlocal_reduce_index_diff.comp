#version 450

layout(local_size_x_id = 0) in;

struct SparseValue {
	uint index;
	float value;
};

layout(constant_id = 1) const uint mat_height = 600;
layout(constant_id = 2) const uint skip_size = 300;

layout(std430, binding = 0) buffer Output2 {
	uint numel;
    uint data[];
};

layout(std430, binding = 1) buffer Input {
    uint scan_io[];
};

uint nearestPowerOfTwo(uint x) {
    //ceil opeartion to nearest 2^x, up to 65536.
    // 255->256, 256->256, 257->512
    x = x - 1u;
    x = x | (x >> 1u);
    x = x | (x >> 2u);
    x = x | (x >> 4u);
    x = x | (x >> 8u);
    x = x | (x >> 16u);
    return x + 1u;
}

#define NEARESTPOW2(x) \
    ((x - 1u) | ((x - 1u) >> 1u) | ((x - 1u) >> 2u) | ((x - 1u) >> 4u) | ((x - 1u) >> 8u) | ((x - 1u) >> 16u) +1u)

shared uint shared_io[gl_WorkGroupSize.x];

void main() {
    uint index = gl_GlobalInvocationID.x;
    uint local_index = gl_LocalInvocationID.x;
    uint work_index = gl_WorkGroupID.x*gl_WorkGroupSize.x;

    if (index*skip_size<mat_height) {
        // shift the built up values left once
        if (local_index>0) {
            scan_io[index*skip_size] = scan_io[index*skip_size+1]-1;
            scan_io[index*skip_size+1]=1;
            data[index*skip_size-1] = data[index*skip_size];

            shared_io[local_index] = scan_io[index*skip_size];

            // Synchronize threads within workgroup
            barrier();

            for (uint stride = nearestPowerOfTwo(gl_WorkGroupSize.x); stride > 0; stride >>= 1) {
                if (shared_io[local_index] >= skip_size) {
                    uint divpow = nearestPowerOfTwo(shared_io[local_index]) >> 1;
                    uint lshift = divpow/skip_size;
                    uint divrem = shared_io[local_index] - divpow;
                    if (int(local_index) - int(lshift) > 0) {
                        shared_io[local_index - lshift] += divrem;  // must be += for the 0 case
                        data[index*skip_size - 1 - divpow] = data[index*skip_size-1];  // only local workgroup will use this, but others may get it. Doesn't matter for later shaders.
                        shared_io[local_index] = divpow;
                    } else {
                        atomicAdd(shared_io[0], divrem);  // acumulate here for higher passes
                        data[work_index*skip_size -1] = data[index*skip_size-1];
                        shared_io[local_index] = divpow;
                    }
                }
                barrier();
            }
        }

        scan_io[index*skip_size] = shared_io[local_index];
    }
}
